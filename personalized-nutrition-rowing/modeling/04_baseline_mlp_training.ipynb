{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3df6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "\n",
    "# --- 配置部分 ---\n",
    "data_path = r\"C:\\Users\\Michael Wang\\OneDrive\\小论文\\毕业论文改写\\WGAN-GP\\建模_数据预处理\\data\\development_set_selected_features.xlsx\"\n",
    "target_column_name = 'Rowing distance'\n",
    "output_plot_path = r\"C:\\Users\\Michael Wang\\OneDrive\\小论文\\毕业论文改写\\WGAN-GP\\插图\"\n",
    "os.makedirs(output_plot_path, exist_ok=True)\n",
    "\n",
    "# 设置随机种子以保证结果可复现\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# 自动选择设备\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --- 数据加载 ---\n",
    "original_data = pd.read_excel(data_path)\n",
    "X_original_df = original_data.drop(columns=[target_column_name])\n",
    "y_original_series = original_data[target_column_name]\n",
    "\n",
    "# --- BPNN/MLP 模型定义 ---\n",
    "class BPNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, output_dim=1, dropout_rate=0.2):\n",
    "        super(BPNN, self).__init__()\n",
    "        layers = []\n",
    "        current_dim = input_dim\n",
    "        for h_dim in hidden_dims:\n",
    "            layers.append(nn.Linear(current_dim, h_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout_rate)) # Dropout for regularization\n",
    "            current_dim = h_dim\n",
    "        layers.append(nn.Linear(current_dim, output_dim))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "# --- 训练和评估函数 ---\n",
    "def train_evaluate_fold(model_config, X_train_fold, y_train_fold, X_val_fold, y_val_fold,\n",
    "                        y_scaler_fold, n_epochs, patience=10):\n",
    "    \"\"\"\n",
    "    训练和评估模型在一个特定的折上。\n",
    "    \"\"\"\n",
    "    input_dim = X_train_fold.shape[1]\n",
    "    model = BPNN(input_dim, model_config['hidden_dims'], dropout_rate=model_config['dropout_rate']).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=model_config['learning_rate'], weight_decay=model_config.get('weight_decay', 0)) # L2正则化\n",
    "    criterion = nn.MSELoss() # 使用MSELoss进行训练，但我们也会监控MAE\n",
    "\n",
    "    # 数据加载器\n",
    "    train_dataset = TensorDataset(torch.FloatTensor(X_train_fold).to(device), torch.FloatTensor(y_train_fold).reshape(-1,1).to(device))\n",
    "    train_loader = DataLoader(train_dataset, batch_size=model_config['batch_size'], shuffle=True)\n",
    "    \n",
    "    val_dataset = TensorDataset(torch.FloatTensor(X_val_fold).to(device), torch.FloatTensor(y_val_fold).reshape(-1,1).to(device))\n",
    "    val_loader = DataLoader(val_dataset, batch_size=model_config['batch_size'], shuffle=False)\n",
    "\n",
    "    epoch_train_mae_unnormalized = []\n",
    "    epoch_val_mae_unnormalized = []\n",
    "    \n",
    "    best_val_loss_mae = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        fold_train_preds_unnorm = []\n",
    "        fold_train_targets_unnorm = []\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # For unnormalized MAE tracking during training\n",
    "            batch_preds_unnorm = y_scaler_fold.inverse_transform(outputs.detach().cpu().numpy())\n",
    "            batch_targets_unnorm = y_scaler_fold.inverse_transform(batch_y.detach().cpu().numpy())\n",
    "            fold_train_preds_unnorm.extend(batch_preds_unnorm.flatten())\n",
    "            fold_train_targets_unnorm.extend(batch_targets_unnorm.flatten())\n",
    "\n",
    "        current_train_mae_unnorm = mean_absolute_error(fold_train_targets_unnorm, fold_train_preds_unnorm)\n",
    "        epoch_train_mae_unnormalized.append(current_train_mae_unnorm)\n",
    "\n",
    "        model.eval()\n",
    "        fold_val_preds_unnorm = []\n",
    "        fold_val_targets_unnorm = []\n",
    "        with torch.no_grad():\n",
    "            for batch_X_val, batch_y_val in val_loader:\n",
    "                val_outputs = model(batch_X_val)\n",
    "                # For unnormalized MAE tracking during validation\n",
    "                batch_val_preds_unnorm = y_scaler_fold.inverse_transform(val_outputs.cpu().numpy())\n",
    "                batch_val_targets_unnorm = y_scaler_fold.inverse_transform(batch_y_val.cpu().numpy())\n",
    "                fold_val_preds_unnorm.extend(batch_val_preds_unnorm.flatten())\n",
    "                fold_val_targets_unnorm.extend(batch_val_targets_unnorm.flatten())\n",
    "        \n",
    "        current_val_mae_unnorm = mean_absolute_error(fold_val_targets_unnorm, fold_val_preds_unnorm)\n",
    "        epoch_val_mae_unnormalized.append(current_val_mae_unnorm)\n",
    "        \n",
    "        # Early stopping based on unnormalized validation MAE\n",
    "        if current_val_mae_unnorm < best_val_loss_mae:\n",
    "            best_val_loss_mae = current_val_mae_unnorm\n",
    "            epochs_no_improve = 0\n",
    "            # torch.save(model.state_dict(), 'best_model_fold.pth') # Optional: save best model for this fold\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "        \n",
    "        if epochs_no_improve >= patience:\n",
    "            # print(f\"Early stopping at epoch {epoch+1} for this fold.\")\n",
    "            break\n",
    "            \n",
    "    # model.load_state_dict(torch.load('best_model_fold.pth')) # Load best model for evaluation\n",
    "\n",
    "    # Final evaluation on validation set for this fold (unnormalized)\n",
    "    model.eval()\n",
    "    all_y_pred_val_scaled = []\n",
    "    all_y_val_scaled = []\n",
    "    with torch.no_grad():\n",
    "        for batch_X_val, batch_y_val in val_loader:\n",
    "            val_outputs = model(batch_X_val)\n",
    "            all_y_pred_val_scaled.extend(val_outputs.cpu().numpy())\n",
    "            all_y_val_scaled.extend(batch_y_val.cpu().numpy())\n",
    "            \n",
    "    y_pred_val_unnorm = y_scaler_fold.inverse_transform(np.array(all_y_pred_val_scaled)).flatten()\n",
    "    y_val_unnorm = y_scaler_fold.inverse_transform(np.array(all_y_val_scaled)).flatten()\n",
    "\n",
    "    mae_val = mean_absolute_error(y_val_unnorm, y_pred_val_unnorm)\n",
    "    mse_val = mean_squared_error(y_val_unnorm, y_pred_val_unnorm)\n",
    "    rmse_val = np.sqrt(mse_val)\n",
    "    r2_val = r2_score(y_val_unnorm, y_pred_val_unnorm)\n",
    "\n",
    "    # Evaluation on training set (for checking overfit, unnormalized)\n",
    "    all_y_pred_train_scaled = []\n",
    "    all_y_train_scaled = []\n",
    "    with torch.no_grad():\n",
    "        for batch_X_train, batch_y_train in train_loader: # Re-iterate train_loader\n",
    "            train_outputs = model(batch_X_train)\n",
    "            all_y_pred_train_scaled.extend(train_outputs.cpu().numpy())\n",
    "            all_y_train_scaled.extend(batch_y_train.cpu().numpy())\n",
    "\n",
    "    y_pred_train_unnorm = y_scaler_fold.inverse_transform(np.array(all_y_pred_train_scaled)).flatten()\n",
    "    y_train_unnorm = y_scaler_fold.inverse_transform(np.array(all_y_train_scaled)).flatten()\n",
    "    \n",
    "    mae_train = mean_absolute_error(y_train_unnorm, y_pred_train_unnorm)\n",
    "    mse_train = mean_squared_error(y_train_unnorm, y_pred_train_unnorm)\n",
    "    rmse_train = np.sqrt(mse_train)\n",
    "    r2_train = r2_score(y_train_unnorm, y_pred_train_unnorm)\n",
    "\n",
    "    return {\n",
    "        'mae_val': mae_val, 'mse_val': mse_val, 'rmse_val': rmse_val, 'r2_val': r2_val,\n",
    "        'mae_train': mae_train, 'mse_train': mse_train, 'rmse_train': rmse_train, 'r2_train': r2_train,\n",
    "        'train_loss_curve': epoch_train_mae_unnormalized,\n",
    "        'val_loss_curve': epoch_val_mae_unnormalized\n",
    "    }\n",
    "\n",
    "\n",
    "# --- 超参数定义 ---\n",
    "# 示例参数网格，可以根据需要调整和扩展\n",
    "param_grid = {\n",
    "    'learning_rate': [0.0005, 0.001, 0.005], # 保持或尝试略小的学习率\n",
    "    'hidden_dims': [\n",
    "        [32],                # 非常简单的单层网络\n",
    "        [64],\n",
    "        [32, 16],            # 两层，但更简单\n",
    "        [64, 32],            # 比之前最佳的略简单\n",
    "        [128, 64, 32]      # 可以保留之前的最佳结构，看增强正则化是否有帮助，或者暂时去掉以强制探索更简单的模型\n",
    "    ],\n",
    "    'batch_size': [16, 32, 64], # 之前最佳是32，可以尝试更大的batch_size看是否能稳定训练或有更好的泛化\n",
    "    'dropout_rate': [0.2, 0.3, 0.4, 0.5], # 显著提高dropout的候选值\n",
    "    'weight_decay': [1e-4, 5e-4, 1e-3, 2e-3] # 尝试引入并增加L2正则化的强度\n",
    "}\n",
    "\n",
    "\n",
    "n_hyperparam_trials = 20 # 随机选择20组超参数进行尝试，可以增加以获得更好的结果\n",
    "n_epochs_hyperparam_tuning = 50 # 用于超参数调整的epoch数，可以减少以加速搜索\n",
    "n_epochs_final_training = 200 # 用于最终模型训练的epoch数\n",
    "patience_hyperparam = 5      # 早停轮数 - 超参数搜索\n",
    "patience_final = 15          # 早停轮数 - 最终模型\n",
    "\n",
    "# --- 超参数调优 (类似RandomizedSearchCV) ---\n",
    "print(\"开始超参数调优...\")\n",
    "best_params = None\n",
    "best_avg_val_mae = float('inf') # 我们以验证集上的平均MAE作为评价标准\n",
    "hyperparam_search_results = []\n",
    "\n",
    "# Convert DataFrame to NumPy arrays for efficiency before loops\n",
    "X_np = X_original_df.values\n",
    "y_np = y_original_series.values\n",
    "\n",
    "# 尝试 n_hyperparam_trials 组随机参数\n",
    "sampled_configs = []\n",
    "for _ in range(n_hyperparam_trials):\n",
    "    config = {}\n",
    "    for key, values in param_grid.items():\n",
    "        config[key] = random.choice(values)\n",
    "    # 避免重复配置 (可选,对于大量试验可能不必要)\n",
    "    if config not in sampled_configs:\n",
    "        sampled_configs.append(config)\n",
    "\n",
    "print(f\"将尝试 {len(sampled_configs)} 组独特的随机超参数组合。\")\n",
    "\n",
    "for i, current_params in enumerate(sampled_configs):\n",
    "    print(f\"\\n--- 超参数组合 {i+1}/{len(sampled_configs)} ---\")\n",
    "    print(current_params)\n",
    "    \n",
    "    kf_hyper = KFold(n_splits=3, shuffle=True, random_state=42) # 使用3折CV加速超参数搜索\n",
    "    fold_val_maes = []\n",
    "\n",
    "    for fold_idx, (train_idx, val_idx) in enumerate(kf_hyper.split(X_np, y_np)):\n",
    "        X_train_fold, X_val_fold = X_np[train_idx], X_np[val_idx]\n",
    "        y_train_fold, y_val_fold = y_np[train_idx], y_np[val_idx]\n",
    "\n",
    "        # 数据归一化 (每折独立，严格来说fit应该只在训练集上，但对于超参数搜索阶段，为简化可以对当前折的X,y都fit_transform)\n",
    "        # 更严格：fit_transform训练集，transform验证集\n",
    "        x_scaler_fold = StandardScaler()\n",
    "        X_train_fold_scaled = x_scaler_fold.fit_transform(X_train_fold)\n",
    "        X_val_fold_scaled = x_scaler_fold.transform(X_val_fold)\n",
    "\n",
    "        y_scaler_fold = StandardScaler() # 用于反归一化输出\n",
    "        y_train_fold_scaled = y_scaler_fold.fit_transform(y_train_fold.reshape(-1, 1)).flatten()\n",
    "        y_val_fold_scaled = y_scaler_fold.transform(y_val_fold.reshape(-1, 1)).flatten()\n",
    "        \n",
    "        # 训练和评估\n",
    "        fold_results = train_evaluate_fold(\n",
    "            current_params, X_train_fold_scaled, y_train_fold_scaled,\n",
    "            X_val_fold_scaled, y_val_fold_scaled, y_scaler_fold, # 传递y_scaler用于反归一化\n",
    "            n_epochs=n_epochs_hyperparam_tuning, patience=patience_hyperparam\n",
    "        )\n",
    "        fold_val_maes.append(fold_results['mae_val'])\n",
    "        # print(f\"  Fold {fold_idx+1} Val MAE: {fold_results['mae_val']:.4f}\")\n",
    "\n",
    "    avg_fold_val_mae = np.mean(fold_val_maes)\n",
    "    hyperparam_search_results.append({'params': current_params, 'avg_val_mae': avg_fold_val_mae})\n",
    "    print(f\"参数组 {i+1} 平均验证 MAE: {avg_fold_val_mae:.4f}\")\n",
    "\n",
    "    if avg_fold_val_mae < best_avg_val_mae:\n",
    "        best_avg_val_mae = avg_fold_val_mae\n",
    "        best_params = current_params\n",
    "\n",
    "print(\"\\n--- 超参数调优完成 ---\")\n",
    "if best_params:\n",
    "    print(f\"最佳参数: {best_params}\")\n",
    "    print(f\"最佳平均验证 MAE: {best_avg_val_mae:.4f}\")\n",
    "else:\n",
    "    print(\"未能找到最佳参数，请检查超参数网格或增加试验次数。将使用第一组参数。\")\n",
    "    best_params = sampled_configs[0]\n",
    "\n",
    "\n",
    "# --- 使用最佳参数进行K折交叉验证评估 (更严格的评估) ---\n",
    "print(\"\\n--- 使用最佳参数进行K折交叉验证评估 ---\")\n",
    "kf_final = KFold(n_splits=5, shuffle=True, random_state=123) # 最终评估使用5折\n",
    "final_fold_metrics = []\n",
    "all_folds_train_loss_curves = []\n",
    "all_folds_val_loss_curves = []\n",
    "\n",
    "for fold_num, (train_index, test_index) in enumerate(kf_final.split(X_np, y_np)):\n",
    "    print(f\"\\nFold {fold_num + 1}/5\")\n",
    "    X_train, X_test = X_np[train_index], X_np[test_index]\n",
    "    y_train, y_test = y_np[train_index], y_np[test_index]\n",
    "\n",
    "    # 数据归一化: Fit on training data, transform both train and test\n",
    "    x_scaler = StandardScaler()\n",
    "    X_train_scaled = x_scaler.fit_transform(X_train)\n",
    "    X_test_scaled = x_scaler.transform(X_test)\n",
    "\n",
    "    y_scaler = StandardScaler() # 用于反归一化目标值\n",
    "    y_train_scaled = y_scaler.fit_transform(y_train.reshape(-1, 1)).flatten()\n",
    "    y_test_scaled = y_scaler.transform(y_test.reshape(-1, 1)).flatten() # 验证集也用训练集fit的scaler\n",
    "\n",
    "    fold_results = train_evaluate_fold(\n",
    "        best_params, X_train_scaled, y_train_scaled,\n",
    "        X_test_scaled, y_test_scaled, y_scaler, # 传递y_scaler\n",
    "        n_epochs=n_epochs_final_training, patience=patience_final\n",
    "    )\n",
    "    \n",
    "    final_fold_metrics.append(fold_results)\n",
    "    all_folds_train_loss_curves.append(fold_results['train_loss_curve'])\n",
    "    all_folds_val_loss_curves.append(fold_results['val_loss_curve'])\n",
    "    \n",
    "    print(f\"  Train MAE: {fold_results['mae_train']:.4f}, Train R2: {fold_results['r2_train']:.4f}\")\n",
    "    print(f\"  Test MAE: {fold_results['mae_val']:.4f}, Test R2: {fold_results['r2_val']:.4f}\")\n",
    "\n",
    "\n",
    "# --- 计算平均性能 ---\n",
    "avg_metrics = {\n",
    "    'mae_train': np.mean([m['mae_train'] for m in final_fold_metrics]),\n",
    "    'mse_train': np.mean([m['mse_train'] for m in final_fold_metrics]),\n",
    "    'rmse_train': np.mean([m['rmse_train'] for m in final_fold_metrics]),\n",
    "    'r2_train': np.mean([m['r2_train'] for m in final_fold_metrics]),\n",
    "    'mae_test': np.mean([m['mae_val'] for m in final_fold_metrics]),\n",
    "    'mse_test': np.mean([m['mse_val'] for m in final_fold_metrics]),\n",
    "    'rmse_test': np.mean([m['rmse_val'] for m in final_fold_metrics]),\n",
    "    'r2_test': np.mean([m['r2_val'] for m in final_fold_metrics]),\n",
    "}\n",
    "\n",
    "print(\"\\nBPNN - Average Train Performance (Unnormalized):\")\n",
    "print(f\"  MAE: {avg_metrics['mae_train']:.4f}\")\n",
    "print(f\"  MSE: {avg_metrics['mse_train']:.4f}\")\n",
    "print(f\"  RMSE: {avg_metrics['rmse_train']:.4f}\")\n",
    "print(f\"  R2 Score: {avg_metrics['r2_train']:.4f}\")\n",
    "\n",
    "print(\"\\nBPNN - Average Test Performance (Unnormalized):\")\n",
    "print(f\"  MAE: {avg_metrics['mae_test']:.4f}\")\n",
    "print(f\"  MSE: {avg_metrics['mse_test']:.4f}\")\n",
    "print(f\"  RMSE: {avg_metrics['rmse_test']:.4f}\")\n",
    "print(f\"  R2 Score: {avg_metrics['r2_test']:.4f}\")\n",
    "\n",
    "\n",
    "# --- 绘制图表并导出 ---\n",
    "\n",
    "# 1. 绘制评估指标图 (MAE 和 R2)\n",
    "metrics_plot_names_en = ['MAE', 'R2 Score']\n",
    "values_train_plot = [avg_metrics['mae_train'], avg_metrics['r2_train']]\n",
    "values_test_plot = [avg_metrics['mae_test'], avg_metrics['r2_test']]\n",
    "\n",
    "x_axis_plot = np.arange(len(metrics_plot_names_en))\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(x_axis_plot - 0.2, values_train_plot, width=0.4, label='Train', align='center')\n",
    "plt.bar(x_axis_plot + 0.2, values_test_plot, width=0.4, label='Test', align='center')\n",
    "plt.xticks(x_axis_plot, metrics_plot_names_en)\n",
    "plt.ylabel('Score')\n",
    "plt.title('BPNN Average Train vs. Test Set Evaluation Metrics')\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plot_filename_metrics = os.path.join(output_plot_path, \"bpnn_average_evaluation_metrics.png\")\n",
    "plt.savefig(plot_filename_metrics, dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# 2. 绘制每折的训练和测试损失 (MAE, 反归一化)\n",
    "plt.figure(figsize=(14, 7))\n",
    "max_epochs_ran = 0\n",
    "for i in range(len(all_folds_train_loss_curves)):\n",
    "    # 由于早停，各折的epoch数可能不同\n",
    "    epochs_this_fold = len(all_folds_train_loss_curves[i])\n",
    "    max_epochs_ran = max(max_epochs_ran, epochs_this_fold)\n",
    "    plt.plot(range(1, epochs_this_fold + 1), all_folds_train_loss_curves[i], label=f'Train Fold {i+1}', linestyle='-')\n",
    "    plt.plot(range(1, epochs_this_fold + 1), all_folds_val_loss_curves[i], label=f'Test Fold {i+1}', linestyle='--')\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Mean Absolute Error (MAE) - Unnormalized')\n",
    "plt.title('BPNN Training and Testing MAE (Unnormalized) per Fold')\n",
    "plt.xlim(1, max_epochs_ran) # 统一X轴范围\n",
    "plt.legend(loc='upper right')\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plot_filename_loss = os.path.join(output_plot_path, \"bpnn_mae_loss_per_fold_unnormalized.png\")\n",
    "plt.savefig(plot_filename_loss, dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n所有图表已尝试保存至: {output_plot_path}\")\n",
    "\n",
    "# --- 过拟合检查 (与XGBoost类似) ---\n",
    "r2_diff = avg_metrics['r2_train'] - avg_metrics['r2_test']\n",
    "mae_train_avg = avg_metrics['mae_train']\n",
    "mae_test_avg = avg_metrics['mae_test']\n",
    "\n",
    "print(\"\\nOverfitting Check:\")\n",
    "if r2_diff > 0.1 and avg_metrics['r2_train'] > avg_metrics['r2_test']:\n",
    "    print(f\"Warning: Model might be overfitting! Train R2 ({avg_metrics['r2_train']:.2f}) is significantly higher than Test R2 ({avg_metrics['r2_test']:.2f}). Difference: {r2_diff:.2f}\")\n",
    "elif mae_train_avg > 0 and (mae_test_avg - mae_train_avg) / mae_train_avg > 0.20 :\n",
    "     print(f\"Warning: Model might be overfitting! Test MAE ({mae_test_avg:.2f}) is >20% higher than Train MAE ({mae_train_avg:.2f}). Relative difference: {((mae_test_avg - mae_train_avg) / mae_train_avg)*100:.2f}%.\")\n",
    "else:\n",
    "    print(f\"No strong signs of overfitting detected based on current thresholds. R2 Diff (Train-Test): {r2_diff:.2f}. MAE Train: {mae_train_avg:.2f}, MAE Test: {mae_test_avg:.2f}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3073fbb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基于原始数据的MLP/BPNN模型 (已添加最终测试集评估)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import KFold, train_test_split # <--- 已在此处添加 train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import joblib\n",
    "\n",
    "# --- 配置部分 ---\n",
    "# 开发集路径 (80%的数据)\n",
    "development_data_path = r\"C:\\Users\\Michael Wang\\OneDrive\\小论文\\毕业论文改写\\WGAN-GP\\建模_数据预处理\\data\\development_set_selected_features.xlsx\"\n",
    "# 最终测试集路径 (20%的数据)\n",
    "final_test_data_path = r\"C:\\Users\\Michael Wang\\OneDrive\\小论文\\毕业论文改写\\WGAN-GP\\建模_数据预处理\\data\\final_test_set_selected_features.xlsx\"\n",
    "\n",
    "target_column_name = 'Rowing distance'\n",
    "output_plot_path = r\"C:\\Users\\Michael Wang\\OneDrive\\小论文\\毕业论文改写\\WGAN-GP\\插图\"\n",
    "os.makedirs(output_plot_path, exist_ok=True)\n",
    "\n",
    "# 设置随机种子以保证结果可复现\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# 自动选择设备\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --- 数据加载 ---\n",
    "original_data = pd.read_excel(development_data_path)\n",
    "X_original_df = original_data.drop(columns=[target_column_name])\n",
    "y_original_series = original_data[target_column_name]\n",
    "print(f\"开发集数据加载成功，形状: {original_data.shape}\")\n",
    "\n",
    "# --- BPNN/MLP 模型定义 ---\n",
    "class BPNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, output_dim=1, dropout_rate=0.2):\n",
    "        super(BPNN, self).__init__()\n",
    "        layers = []\n",
    "        current_dim = input_dim\n",
    "        for h_dim in hidden_dims:\n",
    "            layers.append(nn.Linear(current_dim, h_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout_rate))\n",
    "            current_dim = h_dim\n",
    "        layers.append(nn.Linear(current_dim, output_dim))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "# --- 训练和评估函数 ---\n",
    "def train_evaluate_fold(model_config, X_train_fold, y_train_fold, X_val_fold, y_val_fold,\n",
    "                        y_scaler_fold, n_epochs, patience=10):\n",
    "    input_dim = X_train_fold.shape[1]\n",
    "    model = BPNN(input_dim, model_config['hidden_dims'], dropout_rate=model_config['dropout_rate']).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=model_config['learning_rate'], weight_decay=model_config.get('weight_decay', 0))\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    train_dataset = TensorDataset(torch.FloatTensor(X_train_fold).to(device), torch.FloatTensor(y_train_fold).reshape(-1,1).to(device))\n",
    "    train_loader = DataLoader(train_dataset, batch_size=model_config['batch_size'], shuffle=True)\n",
    "    \n",
    "    val_dataset = TensorDataset(torch.FloatTensor(X_val_fold).to(device), torch.FloatTensor(y_val_fold).reshape(-1,1).to(device))\n",
    "    val_loader = DataLoader(val_dataset, batch_size=model_config['batch_size'], shuffle=False)\n",
    "\n",
    "    epoch_train_mae_unnormalized = []\n",
    "    epoch_val_mae_unnormalized = []\n",
    "    \n",
    "    best_val_loss_mae = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    best_model_state = None\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        # ... (training loop remains the same)\n",
    "        fold_train_preds_unnorm = []\n",
    "        fold_train_targets_unnorm = []\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            batch_preds_unnorm = y_scaler_fold.inverse_transform(outputs.detach().cpu().numpy())\n",
    "            batch_targets_unnorm = y_scaler_fold.inverse_transform(batch_y.detach().cpu().numpy())\n",
    "            fold_train_preds_unnorm.extend(batch_preds_unnorm.flatten())\n",
    "            fold_train_targets_unnorm.extend(batch_targets_unnorm.flatten())\n",
    "        current_train_mae_unnorm = mean_absolute_error(fold_train_targets_unnorm, fold_train_preds_unnorm)\n",
    "        epoch_train_mae_unnormalized.append(current_train_mae_unnorm)\n",
    "\n",
    "        model.eval()\n",
    "        # ... (validation loop remains the same)\n",
    "        fold_val_preds_unnorm = []\n",
    "        fold_val_targets_unnorm = []\n",
    "        with torch.no_grad():\n",
    "            for batch_X_val, batch_y_val in val_loader:\n",
    "                val_outputs = model(batch_X_val)\n",
    "                batch_val_preds_unnorm = y_scaler_fold.inverse_transform(val_outputs.cpu().numpy())\n",
    "                batch_val_targets_unnorm = y_scaler_fold.inverse_transform(batch_y_val.cpu().numpy())\n",
    "                fold_val_preds_unnorm.extend(batch_val_preds_unnorm.flatten())\n",
    "                fold_val_targets_unnorm.extend(batch_val_targets_unnorm.flatten())\n",
    "        current_val_mae_unnorm = mean_absolute_error(fold_val_targets_unnorm, fold_val_preds_unnorm)\n",
    "        epoch_val_mae_unnormalized.append(current_val_mae_unnorm)\n",
    "        \n",
    "        if current_val_mae_unnorm < best_val_loss_mae:\n",
    "            best_val_loss_mae = current_val_mae_unnorm\n",
    "            epochs_no_improve = 0\n",
    "            best_model_state = model.state_dict()\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "        \n",
    "        if epochs_no_improve >= patience:\n",
    "            break\n",
    "            \n",
    "    if best_model_state:\n",
    "        model.load_state_dict(best_model_state)\n",
    "\n",
    "    # Final evaluation on validation set for this fold\n",
    "    model.eval()\n",
    "    # ... (evaluation logic remains the same)\n",
    "    all_y_pred_val_scaled = []\n",
    "    all_y_val_scaled = []\n",
    "    with torch.no_grad():\n",
    "        for batch_X_val, batch_y_val in val_loader:\n",
    "            val_outputs = model(batch_X_val)\n",
    "            all_y_pred_val_scaled.extend(val_outputs.cpu().numpy())\n",
    "            all_y_val_scaled.extend(batch_y_val.cpu().numpy())\n",
    "    y_pred_val_unnorm = y_scaler_fold.inverse_transform(np.array(all_y_pred_val_scaled)).flatten()\n",
    "    y_val_unnorm = y_scaler_fold.inverse_transform(np.array(all_y_val_scaled)).flatten()\n",
    "    mae_val = mean_absolute_error(y_val_unnorm, y_pred_val_unnorm)\n",
    "    rmse_val = np.sqrt(mean_squared_error(y_val_unnorm, y_pred_val_unnorm))\n",
    "    r2_val = r2_score(y_val_unnorm, y_pred_val_unnorm)\n",
    "\n",
    "    # Evaluation on training set\n",
    "    all_y_pred_train_scaled = []\n",
    "    all_y_train_scaled = []\n",
    "    with torch.no_grad():\n",
    "        for batch_X_train, batch_y_train in train_loader:\n",
    "            train_outputs = model(batch_X_train)\n",
    "            all_y_pred_train_scaled.extend(train_outputs.cpu().numpy())\n",
    "            all_y_train_scaled.extend(batch_y_train.cpu().numpy())\n",
    "    y_pred_train_unnorm = y_scaler_fold.inverse_transform(np.array(all_y_pred_train_scaled)).flatten()\n",
    "    y_train_unnorm = y_scaler_fold.inverse_transform(np.array(all_y_train_scaled)).flatten()\n",
    "    mae_train = mean_absolute_error(y_train_unnorm, y_pred_train_unnorm)\n",
    "    rmse_train = np.sqrt(mean_squared_error(y_train_unnorm, y_pred_train_unnorm))\n",
    "    r2_train = r2_score(y_train_unnorm, y_pred_train_unnorm)\n",
    "\n",
    "    return {\n",
    "        'mae_val': mae_val, 'rmse_val': rmse_val, 'r2_val': r2_val,\n",
    "        'mae_train': mae_train, 'rmse_train': rmse_train, 'r2_train': r2_train,\n",
    "        'train_loss_curve': epoch_train_mae_unnormalized,\n",
    "        'val_loss_curve': epoch_val_mae_unnormalized,\n",
    "        'model_state': best_model_state\n",
    "    }\n",
    "\n",
    "\n",
    "# --- 超参数定义 ---\n",
    "param_grid = {\n",
    "    'learning_rate': [0.0005, 0.001, 0.005],\n",
    "    'hidden_dims': [[32], [64], [32, 16], [64, 32], [128, 64, 32]],\n",
    "    'batch_size': [16, 32, 64],\n",
    "    'dropout_rate': [0.2, 0.3, 0.4, 0.5],\n",
    "    'weight_decay': [1e-4, 5e-4, 1e-3, 2e-3]\n",
    "}\n",
    "n_hyperparam_trials = 20\n",
    "n_epochs_hyperparam_tuning = 50\n",
    "patience_hyperparam = 5\n",
    "n_epochs_final_training = 200\n",
    "patience_final = 15\n",
    "\n",
    "# --- 超参数调优 (在开发集上) ---\n",
    "print(\"开始超参数调优...\")\n",
    "# ... (Hyperparameter tuning logic remains the same) ...\n",
    "best_params = None\n",
    "best_avg_val_mae = float('inf')\n",
    "X_np = X_original_df.values\n",
    "y_np = y_original_series.values\n",
    "sampled_configs = []\n",
    "for _ in range(n_hyperparam_trials):\n",
    "    config = {}\n",
    "    for key, values in param_grid.items():\n",
    "        config[key] = random.choice(values)\n",
    "    if config not in sampled_configs:\n",
    "        sampled_configs.append(config)\n",
    "\n",
    "for i, current_params in enumerate(sampled_configs):\n",
    "    print(f\"\\n--- 超参数组合 {i+1}/{len(sampled_configs)} ---\")\n",
    "    print(current_params)\n",
    "    kf_hyper = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    fold_val_maes = []\n",
    "    for fold_idx, (train_idx, val_idx) in enumerate(kf_hyper.split(X_np, y_np)):\n",
    "        X_train_fold, X_val_fold = X_np[train_idx], X_np[val_idx]\n",
    "        y_train_fold, y_val_fold = y_np[train_idx], y_np[val_idx]\n",
    "        x_scaler_fold = StandardScaler()\n",
    "        X_train_fold_scaled = x_scaler_fold.fit_transform(X_train_fold)\n",
    "        X_val_fold_scaled = x_scaler_fold.transform(X_val_fold)\n",
    "        y_scaler_fold = StandardScaler()\n",
    "        y_train_fold_scaled = y_scaler_fold.fit_transform(y_train_fold.reshape(-1, 1)).flatten()\n",
    "        y_val_fold_scaled = y_scaler_fold.transform(y_val_fold.reshape(-1, 1)).flatten()\n",
    "        fold_results = train_evaluate_fold(\n",
    "            current_params, X_train_fold_scaled, y_train_fold_scaled,\n",
    "            X_val_fold_scaled, y_val_fold_scaled, y_scaler_fold,\n",
    "            n_epochs=n_epochs_hyperparam_tuning, patience=patience_hyperparam\n",
    "        )\n",
    "        fold_val_maes.append(fold_results['mae_val'])\n",
    "    avg_fold_val_mae = np.mean(fold_val_maes)\n",
    "    print(f\"参数组 {i+1} 平均验证 MAE: {avg_fold_val_mae:.4f}\")\n",
    "    if avg_fold_val_mae < best_avg_val_mae:\n",
    "        best_avg_val_mae = avg_fold_val_mae\n",
    "        best_params = current_params\n",
    "\n",
    "print(\"\\n--- 超参数调优完成 ---\")\n",
    "if not best_params: best_params = sampled_configs[0]\n",
    "print(f\"最佳参数: {best_params}\")\n",
    "print(f\"最佳平均验证 MAE: {best_avg_val_mae:.4f}\")\n",
    "\n",
    "# --- 使用最佳参数进行K折交叉验证评估 (在开发集上) ---\n",
    "print(\"\\n--- 使用最佳参数进行K折交叉验证评估 (在开发集内部) ---\")\n",
    "# ... (K-fold CV logic remains the same) ...\n",
    "kf_final = KFold(n_splits=5, shuffle=True, random_state=123)\n",
    "final_fold_metrics = []\n",
    "for fold_num, (train_index, test_index) in enumerate(kf_final.split(X_np, y_np)):\n",
    "    print(f\"\\nFold {fold_num + 1}/5\")\n",
    "    X_train, X_test = X_np[train_index], X_np[test_index]\n",
    "    y_train, y_test = y_np[train_index], y_np[test_index]\n",
    "    x_scaler = StandardScaler()\n",
    "    X_train_scaled = x_scaler.fit_transform(X_train)\n",
    "    X_test_scaled = x_scaler.transform(X_test)\n",
    "    y_scaler = StandardScaler()\n",
    "    y_train_scaled = y_scaler.fit_transform(y_train.reshape(-1, 1)).flatten()\n",
    "    y_test_scaled = y_scaler.transform(y_test.reshape(-1, 1)).flatten()\n",
    "    fold_results = train_evaluate_fold(\n",
    "        best_params, X_train_scaled, y_train_scaled,\n",
    "        X_test_scaled, y_test_scaled, y_scaler,\n",
    "        n_epochs=n_epochs_final_training, patience=patience_final\n",
    "    )\n",
    "    final_fold_metrics.append(fold_results)\n",
    "    print(f\"  CV-Train MAE: {fold_results['mae_train']:.4f}, R2: {fold_results['r2_train']:.4f}\")\n",
    "    print(f\"  CV-Validation MAE: {fold_results['mae_val']:.4f}, R2: {fold_results['r2_val']:.4f}\")\n",
    "\n",
    "# --- 计算交叉验证的平均性能 ---\n",
    "avg_metrics = {\n",
    "    'mae_train': np.mean([m['mae_train'] for m in final_fold_metrics]),\n",
    "    'rmse_train': np.mean([m['rmse_train'] for m in final_fold_metrics]),\n",
    "    'r2_train': np.mean([m['r2_train'] for m in final_fold_metrics]),\n",
    "    'mae_val': np.mean([m['mae_val'] for m in final_fold_metrics]),\n",
    "    'rmse_val': np.mean([m['rmse_val'] for m in final_fold_metrics]),\n",
    "    'r2_val': np.mean([m['r2_val'] for m in final_fold_metrics]),\n",
    "}\n",
    "\n",
    "print(\"\\nBPNN - Average CV Train Performance (on Development Set, Unnormalized):\")\n",
    "print(f\"  MAE: {avg_metrics['mae_train']:.4f}, RMSE: {avg_metrics['rmse_train']:.4f}, R2 Score: {avg_metrics['r2_train']:.4f}\")\n",
    "\n",
    "print(\"\\nBPNN - Average CV Validation Performance (on Development Set, Unnormalized):\")\n",
    "print(f\"  MAE: {avg_metrics['mae_val']:.4f}, RMSE: {avg_metrics['rmse_val']:.4f}, R2 Score: {avg_metrics['r2_val']:.4f}\")\n",
    "\n",
    "# --- 训练最终的基准MLP模型 (在整个开发集上) ---\n",
    "print(\"\\n--- 训练最终基准MLP模型 (在整个80%开发集上) ---\")\n",
    "# 1. 创建并拟合最终的scalers\n",
    "final_x_scaler = StandardScaler().fit(X_original_df.values)\n",
    "final_y_scaler = StandardScaler().fit(y_original_series.values.reshape(-1, 1))\n",
    "\n",
    "# 2. 缩放整个开发集\n",
    "X_dev_scaled = final_x_scaler.transform(X_original_df.values)\n",
    "y_dev_scaled = final_y_scaler.transform(y_original_series.values.reshape(-1, 1)).flatten()\n",
    "\n",
    "# 3. 训练最终模型\n",
    "# 为了使用早停，我们从开发集中临时划分一小部分作为验证集\n",
    "X_final_train_s, X_final_val_s, y_final_train_s, y_final_val_s = train_test_split(\n",
    "    X_dev_scaled, y_dev_scaled, test_size=0.1, random_state=42\n",
    ")\n",
    "\n",
    "final_model_results = train_evaluate_fold(\n",
    "    best_params, X_final_train_s, y_final_train_s,\n",
    "    X_final_val_s, y_final_val_s, final_y_scaler, # 传递final_y_scaler\n",
    "    n_epochs=n_epochs_final_training, patience=patience_final\n",
    ")\n",
    "print(\"最终基准MLP模型训练完成。\")\n",
    "\n",
    "# ==============================================================================\n",
    "# ======================== 新增的最终评估代码块开始 ========================\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n--- 最终无偏评估 (在20%最终留出测试集上) ---\")\n",
    "\n",
    "# --- 1. 加载最终测试集数据 ---\n",
    "try:\n",
    "    final_test_df = pd.read_excel(final_test_data_path)\n",
    "    print(f\"最终测试集数据加载成功，形状: {final_test_df.shape}\")\n",
    "    X_final_test_df = final_test_df.drop(columns=[target_column_name])\n",
    "    y_final_test_series = final_test_df[target_column_name]\n",
    "except FileNotFoundError:\n",
    "    print(f\"错误: 最终测试集文件未找到: {final_test_data_path}\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"加载最终测试集时发生错误: {e}\")\n",
    "    exit()\n",
    "\n",
    "# --- 2. 加载最终模型并准备测试数据 ---\n",
    "# 创建一个新的模型实例并加载最佳状态\n",
    "final_model_for_test = BPNN(X_original_df.shape[1], best_params['hidden_dims'], dropout_rate=best_params['dropout_rate']).to(device)\n",
    "final_model_for_test.load_state_dict(final_model_results['model_state'])\n",
    "final_model_for_test.eval()\n",
    "\n",
    "# 使用在整个开发集上fit的scaler来转换测试数据\n",
    "X_final_test_scaled = final_x_scaler.transform(X_final_test_df.values)\n",
    "X_final_test_tensor = torch.FloatTensor(X_final_test_scaled).to(device)\n",
    "\n",
    "# --- 3. 进行预测并反归一化 ---\n",
    "with torch.no_grad():\n",
    "    y_pred_final_test_scaled_tensor = final_model_for_test(X_final_test_tensor)\n",
    "\n",
    "y_pred_final_test_scaled_np = y_pred_final_test_scaled_tensor.cpu().numpy()\n",
    "y_pred_final_test_unnorm = final_y_scaler.inverse_transform(y_pred_final_test_scaled_np).flatten()\n",
    "\n",
    "# --- 4. 计算并打印最终性能指标 ---\n",
    "mae_final_bpnn = mean_absolute_error(y_final_test_series.values, y_pred_final_test_unnorm)\n",
    "rmse_final_bpnn = np.sqrt(mean_squared_error(y_final_test_series.values, y_pred_final_test_unnorm))\n",
    "r2_final_bpnn = r2_score(y_final_test_series.values, y_pred_final_test_unnorm)\n",
    "\n",
    "print(\"\\n--- 最终BPNN模型在最终测试集上的性能 ---\")\n",
    "print(f\"MAE: {mae_final_bpnn:.4f}\")\n",
    "print(f\"RMSE: {rmse_final_bpnn:.4f}\")\n",
    "print(f\"R2 Score: {r2_final_bpnn:.4f}\")\n",
    "\n",
    "# (可选) 绘制真实值 vs 预测值图\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.scatter(y_final_test_series.values, y_pred_final_test_unnorm, alpha=0.7, edgecolors='w', linewidth=0.5, color='green')\n",
    "min_val = min(y_final_test_series.min(), y_pred_final_test_unnorm.min())\n",
    "max_val = max(y_final_test_series.max(), y_pred_final_test_unnorm.max())\n",
    "plt.plot([min_val, max_val], [min_val, max_val], 'k--', lw=2)\n",
    "plt.xlabel('Actual Rowing Distance')\n",
    "plt.ylabel('Predicted Rowing Distance (MLP)')\n",
    "plt.title('MLP Baseline Model: Actual vs. Predicted (Hold-Out Test Set)')\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plot_filename_actual_vs_pred_mlp = os.path.join(output_plot_path, \"mlp_baseline_model_actual_vs_predicted.png\")\n",
    "try:\n",
    "    plt.savefig(plot_filename_actual_vs_pred_mlp, dpi=300, bbox_inches='tight')\n",
    "    print(f\"\\nMLP基准模型真实值 vs 预测值图已保存到: {plot_filename_actual_vs_pred_mlp}\")\n",
    "except Exception as e:\n",
    "    print(f\"保存MLP基准模型真实值 vs 预测值图时发生错误: {e}\")\n",
    "plt.show()\n",
    "\n",
    "# ==============================================================================\n",
    "# ========================= 新增的最终评估代码块结束 =========================\n",
    "# ==============================================================================\n",
    "\n",
    "# 原有的图表绘制部分可以继续保留，它们是基于CV结果的\n",
    "# ... (此处省略了原有图表绘制代码，逻辑不变) ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da58061-e2ac-4691-9a3c-6739125405db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置目标保存路径\n",
    "target_path = r\"C:\\Users\\Michael Wang\\OneDrive\\小论文\\毕业论文改写\\WGAN-GP\\插图\\返修插图\"\n",
    "\n",
    "# 确保目录存在，如果不存在则创建\n",
    "import os\n",
    "os.makedirs(target_path, exist_ok=True)\n",
    "\n",
    "# 1. 绘制评估指标图 (MAE 和 R2)\n",
    "metrics_plot_names_en = ['MAE', 'R2 Score']\n",
    "values_train_plot = [avg_metrics['mae_train'], avg_metrics['r2_train']]\n",
    "values_test_plot = [avg_metrics['mae_val'], avg_metrics['r2_val']]\n",
    "x_axis_plot = np.arange(len(metrics_plot_names_en))\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(x_axis_plot - 0.2, values_train_plot, width=0.4, label='CV Train', align='center')\n",
    "plt.bar(x_axis_plot + 0.2, values_test_plot, width=0.4, label='CV Validation', align='center')\n",
    "plt.xticks(x_axis_plot, metrics_plot_names_en)\n",
    "plt.ylabel('Score')\n",
    "plt.title('BPNN Average CV Evaluation Metrics (Original Data)')\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "# 修改保存路径和文件名，添加前缀\n",
    "plot_filename_metrics = os.path.join(target_path, \"MLP原始数据_bpnn_average_evaluation_metrics.png\")\n",
    "plt.savefig(plot_filename_metrics, dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# 2. 绘制每折的训练和测试损失 (MAE, 反归一化)\n",
    "all_folds_train_loss_curves = [m['train_loss_curve'] for m in final_fold_metrics]\n",
    "all_folds_val_loss_curves = [m['val_loss_curve'] for m in final_fold_metrics]\n",
    "plt.figure(figsize=(14, 7))\n",
    "max_epochs_ran = 0\n",
    "for i in range(len(all_folds_train_loss_curves)):\n",
    "    epochs_this_fold = len(all_folds_train_loss_curves[i])\n",
    "    max_epochs_ran = max(max_epochs_ran, epochs_this_fold)\n",
    "    plt.plot(range(1, epochs_this_fold + 1), all_folds_train_loss_curves[i], label=f'CV Train Fold {i+1}', linestyle='-')\n",
    "    plt.plot(range(1, epochs_this_fold + 1), all_folds_val_loss_curves[i], label=f'CV Validation Fold {i+1}', linestyle='--')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Mean Absolute Error (MAE) - Unnormalized')\n",
    "plt.title('BPNN Training and CV Validation MAE (Unnormalized) per Fold')\n",
    "if max_epochs_ran > 0: plt.xlim(1, max_epochs_ran)\n",
    "plt.legend(loc='upper right')\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "# 修改保存路径和文件名，添加前缀\n",
    "plot_filename_loss = os.path.join(target_path, \"MLP原始数据_bpnn_mae_loss_per_fold_unnormalized.png\")\n",
    "plt.savefig(plot_filename_loss, dpi=300, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6cf4b61-742e-423b-8a63-f7eebd2ad9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (可选) 绘制真实值 vs 预测值图\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.scatter(y_final_test_series.values, y_pred_final_test_unnorm, alpha=0.7, edgecolors='w', linewidth=0.5)\n",
    "min_val = min(y_final_test_series.min(), y_pred_final_test_unnorm.min())\n",
    "max_val = max(y_final_test_series.max(), y_pred_final_test_unnorm.max())\n",
    "plt.plot([min_val, max_val], [min_val, max_val], 'k--', lw=2)\n",
    "plt.xlabel('Actual Rowing Distance')\n",
    "plt.ylabel('Predicted Rowing Distance (MLP)')\n",
    "plt.title('MLP Baseline Model: Actual vs. Predicted (Hold-Out Test Set)')\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plot_filename_actual_vs_pred_mlp = os.path.join(output_plot_path, \"mlp_baseline_model_actual_vs_predicted.png\")\n",
    "try:\n",
    "    plt.savefig(plot_filename_actual_vs_pred_mlp, dpi=300, bbox_inches='tight')\n",
    "    print(f\"\\nMLP基准模型真实值 vs 预测值图已保存到: {plot_filename_actual_vs_pred_mlp}\")\n",
    "except Exception as e:\n",
    "    print(f\"保存MLP基准模型真实值 vs 预测值图时发生错误: {e}\")\n",
    "plt.show()\n",
    "# 修改保存路径和文件名，添加前缀\n",
    "plot_filename_loss = os.path.join(target_path, \"MLP原始数据_mlp_baseline_model_actual_vs_predicted.png\")\n",
    "plt.savefig(plot_filename_loss, dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d05b90-35c6-4762-9d2d-8aca86de5d7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (xgb_gpu_env)",
   "language": "python",
   "name": "xgb_gpu_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
