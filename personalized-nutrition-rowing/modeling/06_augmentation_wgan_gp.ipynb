{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cca443a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt # 暂时保留，虽然此代码不主动绘图，但调试时可能有用\n",
    "import os\n",
    "from pathlib import Path \n",
    "CURRENT_DIR = Path.cwd()\n",
    "PROJECT_ROOT = CURRENT_DIR.parent\n",
    "DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "OUTPUT_DIR = PROJECT_ROOT / \"output\"\n",
    "\n",
    "# --- 1. 配置参数 ---\n",
    "# 数据路径 (你的 \"data1.xlsx\" 包含20个输入特征 + 1个目标 \"Rowing distance\")\n",
    "data_file_path = DATA_DIR / \"development_set_selected_features.xlsx\"\n",
    "# 输出增强数据的文件路径\n",
    "output_folder_path = DATA_DIR\n",
    "output_file_name = \"development_set_selected_features_迭代10000.xlsx\"\n",
    "full_output_path = os.path.join(output_folder_path, output_file_name)\n",
    "\n",
    "\n",
    "# GAN 超参数\n",
    "latent_dim = 100  # 潜变量维度\n",
    "lambda_gp = 10    # 梯度惩罚的系数\n",
    "n_critic = 5      # 每轮训练生成器一次，训练判别器的次数\n",
    "epochs = 10000    # 训练轮数 (对于GAN，通常需要较多轮次，请根据实际情况调整)\n",
    "batch_size = 32   # 批处理大小 (可根据你的数据量和内存调整，171条数据，32或64均可)\n",
    "lr = 0.00005      # 学习率\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"将使用设备: {device}\")\n",
    "\n",
    "# --- 2. 数据加载和预处理 ---\n",
    "try:\n",
    "    original_data_df = pd.read_excel(data_file_path)\n",
    "    print(f\"原始数据已从 '{data_file_path}' 加载，形状: {original_data_df.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"错误: 文件 '{data_file_path}' 未找到。请检查路径。\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"加载数据时发生错误: {e}\")\n",
    "    exit()\n",
    "\n",
    "# 获取所有列名 (这21列都将作为连续特征处理)\n",
    "all_feature_names = original_data_df.columns.tolist()\n",
    "print(f\"所有特征 ({len(all_feature_names)}): {all_feature_names}\")\n",
    "\n",
    "# 将所有特征转换为float32 numpy数组\n",
    "original_data_values = original_data_df.values.astype(np.float32)\n",
    "\n",
    "# (A) 标准化所有特征\n",
    "data_mean = np.mean(original_data_values, axis=0)\n",
    "data_std = np.std(original_data_values, axis=0)\n",
    "# 防止标准差为0的情况 (如果某列值都相同)\n",
    "data_std[data_std == 0] = 1 \n",
    "standardized_data = (original_data_values - data_mean) / data_std\n",
    "\n",
    "# (B) 创建DataLoader\n",
    "data_tensor = torch.tensor(standardized_data)\n",
    "dataset = TensorDataset(data_tensor)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True) # drop_last=True 如果样本数不能被batch_size整除\n",
    "\n",
    "# --- 3. 定义GAN模型 (Generator 和 Critic) ---\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, output_dim) # 输出层没有激活函数，以生成连续值\n",
    "        )\n",
    "    def forward(self, z):\n",
    "        return self.model(z)\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Critic, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 512),\n",
    "            nn.LeakyReLU(0.2, inplace=True), # LeakyReLU 通常在判别器中效果更好\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(128, 1) # 输出一个标量评分\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# --- 4. 定义梯度惩罚函数 ---\n",
    "def gradient_penalty(critic_model, real_samples, fake_samples, device_in_use):\n",
    "    batch_size_gp = real_samples.size(0)\n",
    "    # Alpha L ~ U[0,1] for each sample in batch\n",
    "    alpha = torch.rand(batch_size_gp, 1, device=device_in_use)\n",
    "    alpha = alpha.expand_as(real_samples) # Expand to real_samples size\n",
    "\n",
    "    interpolates = (alpha * real_samples + ((1 - alpha) * fake_samples)).requires_grad_(True)\n",
    "    d_interpolates = critic_model(interpolates)\n",
    "\n",
    "    # Use a \"fake\" gradient tensor of all ones\n",
    "    fake_grad_output = torch.ones_like(d_interpolates, device=device_in_use)\n",
    "\n",
    "    gradients = torch.autograd.grad(\n",
    "        outputs=d_interpolates,\n",
    "        inputs=interpolates,\n",
    "        grad_outputs=fake_grad_output,\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "        only_inputs=True\n",
    "    )[0]\n",
    "    gradients = gradients.view(gradients.size(0), -1) # Flatten\n",
    "    gradient_penalty_val = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "    return gradient_penalty_val\n",
    "\n",
    "# --- 5. 初始化模型、优化器 ---\n",
    "num_features = standardized_data.shape[1] # 应该是21\n",
    "\n",
    "generator = Generator(latent_dim, num_features).to(device)\n",
    "critic = Critic(num_features).to(device)\n",
    "\n",
    "optimizer_G = optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.9)) # Adam通常比RMSprop更稳定\n",
    "optimizer_C = optim.Adam(critic.parameters(), lr=lr, betas=(0.5, 0.9))\n",
    "\n",
    "# --- 6. 训练循环 ---\n",
    "print(\"\\n开始WGAN-GP训练...\")\n",
    "for epoch in range(epochs):\n",
    "    for i, (real_samples_batch,) in enumerate(dataloader):\n",
    "        real_samples_batch = real_samples_batch.to(device)\n",
    "        current_batch_size = real_samples_batch.size(0)\n",
    "\n",
    "        # ---------------------\n",
    "        #  训练判别器 (Critic)\n",
    "        # ---------------------\n",
    "        for _ in range(n_critic):\n",
    "            optimizer_C.zero_grad()\n",
    "\n",
    "            # 从潜空间采样噪声作为生成器输入\n",
    "            z = torch.randn(current_batch_size, latent_dim, device=device)\n",
    "            fake_samples_batch = generator(z)\n",
    "\n",
    "            # 计算判别器对真实样本和伪造样本的评分\n",
    "            critic_real = critic(real_samples_batch)\n",
    "            critic_fake = critic(fake_samples_batch.detach()) # detach以避免更新生成器\n",
    "\n",
    "            # 计算梯度惩罚\n",
    "            gp = gradient_penalty(critic, real_samples_batch, fake_samples_batch, device)\n",
    "\n",
    "            # 判别器损失 (Wasserstein距离 + 梯度惩罚)\n",
    "            critic_loss = torch.mean(critic_fake) - torch.mean(critic_real) + lambda_gp * gp\n",
    "            \n",
    "            critic_loss.backward()\n",
    "            optimizer_C.step()\n",
    "\n",
    "        # -----------------\n",
    "        #  训练生成器\n",
    "        # -----------------\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        # 生成一批新的伪造样本\n",
    "        z = torch.randn(current_batch_size, latent_dim, device=device)\n",
    "        generated_for_g_loss = generator(z)\n",
    "\n",
    "        # 生成器损失 (试图让判别器认为伪造样本是真实的)\n",
    "        generator_loss = -torch.mean(critic(generated_for_g_loss))\n",
    "\n",
    "        generator_loss.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "    # 打印训练过程中的损失\n",
    "    if (epoch + 1) % 200 == 0: # 每200轮打印一次\n",
    "        print(\n",
    "            f\"[Epoch {epoch+1}/{epochs}] \"\n",
    "            f\"[Critic Loss: {critic_loss.item():.4f}] \"\n",
    "            f\"[Generator Loss: {generator_loss.item():.4f}]\"\n",
    "        )\n",
    "\n",
    "print(\"训练完成。\")\n",
    "\n",
    "# --- 7. 生成新数据并进行后处理 ---\n",
    "print(\"\\n正在生成增强数据...\")\n",
    "# 设置需要生成的样本数量\n",
    "# num_generated_samples = original_data_df.shape[0] # 例如，生成与原始数据同样多的样本\n",
    "# 或者你可以设置一个更大的数量，例如：\n",
    "num_generated_samples = 2000\n",
    "\n",
    "generator.eval() # 将生成器设置为评估模式\n",
    "with torch.no_grad():\n",
    "    z_generate = torch.randn(num_generated_samples, latent_dim, device=device)\n",
    "    generated_standardized_data_np = generator(z_generate).detach().cpu().numpy()\n",
    "\n",
    "# (A) 反标准化所有生成的特征\n",
    "generated_data_original_scale_np = generated_standardized_data_np * data_std + data_mean\n",
    "generated_data_df = pd.DataFrame(generated_data_original_scale_np, columns=all_feature_names)\n",
    "\n",
    "# # (B) 对 'CHO' 列进行特殊后处理：映射到最近的预设等级\n",
    "# if 'CHO' in generated_data_df.columns:\n",
    "#     print(\"正在对 'CHO' 列进行后处理...\")\n",
    "#     cho_column_generated = generated_data_df['CHO'].values\n",
    "#     # 找到每个生成值最近的CHO等级\n",
    "#     processed_cho = np.array([CHO_LEVELS[np.abs(CHO_LEVELS - val).argmin()] for val in cho_column_generated])\n",
    "#     generated_data_df['CHO'] = processed_cho\n",
    "#     print(\"'CHO' 列处理完成。\")\n",
    "# else:\n",
    "#     print(\"警告: 'CHO' 列未在生成数据中找到，跳过CHO特定后处理。\")\n",
    "\n",
    "\n",
    "# (C) 对其他所有连续特征 (包括 'Rowing distance'，但不包括已特殊处理的 'CHO') 进行np.clip\n",
    "print(\"正在对其他连续特征进行范围裁剪...\")\n",
    "for col_name in all_feature_names:\n",
    "    # if col_name == 'CHO': # CHO已经特殊处理过了\n",
    "    #     continue\n",
    "\n",
    "    original_col_values = original_data_df[col_name]\n",
    "    col_min_original = original_col_values.min()\n",
    "    col_max_original = original_col_values.max()\n",
    "    col_range = col_max_original - col_min_original\n",
    "\n",
    "    if col_range == 0: # 如果原始列中所有值都相同\n",
    "        clip_min_for_col = col_min_original\n",
    "        clip_max_for_col = col_max_original\n",
    "    else:\n",
    "        clip_min_for_col = col_min_original - 0.01 * col_range\n",
    "        clip_max_for_col = col_max_original + 0.01 * col_range\n",
    "            # 确保 CHO (以及其他相关营养素) 在生理上不可能为负值的情况下不被裁剪到0以下\n",
    "    if col_name in ['CHO', 'PRO']: # 如果需要，可以加入其他相关的营养素名称\n",
    "        clip_min_for_col = max(0, clip_min_for_col) # 例如，CHO和PRO不应小于0\n",
    "    \n",
    "    # 应用裁剪\n",
    "    generated_data_df[col_name] = np.clip(generated_data_df[col_name], clip_min_for_col, clip_max_for_col)\n",
    "    # print(f\"  列 '{col_name}' 已裁剪至范围 [{clip_min_for_col:.2f}, {clip_max_for_col:.2f}]\")\n",
    "\n",
    "print(\"所有其他连续特征裁剪完成。\")\n",
    "\n",
    "\n",
    "# --- 8. 保存生成的增强数据 ---\n",
    "try:\n",
    "    os.makedirs(output_folder_path, exist_ok=True) # 再次确保文件夹存在\n",
    "    generated_data_df.to_excel(full_output_path, index=False)\n",
    "    print(f\"\\n增强数据已成功保存到: {full_output_path}\")\n",
    "    print(f\"生成数据形状: {generated_data_df.shape}\")\n",
    "    print(\"生成数据前5行:\")\n",
    "    print(generated_data_df.head())\n",
    "except Exception as e:\n",
    "    print(f\"保存增强数据时发生错误: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8858eb35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (xgb_gpu_env)",
   "language": "python",
   "name": "xgb_gpu_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
