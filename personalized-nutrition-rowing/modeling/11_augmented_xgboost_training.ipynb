{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac923140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgboost + wgan-gp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import KFold, RandomizedSearchCV, train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import joblib\n",
    "from pathlib import Path \n",
    "CURRENT_DIR = Path.cwd()\n",
    "PROJECT_ROOT = CURRENT_DIR.parent\n",
    "DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "OUTPUT_DIR = PROJECT_ROOT / \"output\"\n",
    "\n",
    "# --- 0. 全局配置 ---\n",
    "# 数据路径\n",
    "BASE_DATA_PATH = DATA_DIR\n",
    "DEV_SET_FILE = DATA_DIR / \"development_set_selected_features.xlsx\"\n",
    "TEST_SET_FILE = DATA_DIR / \"final_test_set_selected_features.xlsx\"\n",
    "AUGMENTED_DATA_OUTPUT_FOLDER =  DATA_DIR / \"augmented_outputs\" # 保存中间增强数据\n",
    "MODEL_OUTPUT_PATH = DATA_DIR / \"trained_models\" # 保存训练好的模型\n",
    "OUTPUT_PLOT_PATH = OUTPUT_DIR # 图表导出路径\n",
    "\n",
    "os.makedirs(AUGMENTED_DATA_OUTPUT_FOLDER, exist_ok=True)\n",
    "os.makedirs(MODEL_OUTPUT_PATH, exist_ok=True)\n",
    "os.makedirs(OUTPUT_PLOT_PATH, exist_ok=True) # 确保图表路径存在\n",
    "\n",
    "TARGET_COLUMN = 'Rowing distance'\n",
    "RANDOM_STATE = 42\n",
    "N_SPLITS_KFOLD = 5 # K-Fold的折数\n",
    "\n",
    "# WGAN-GP 预设参数\n",
    "# CHO_LEVELS = np.array([0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 1.1, 1.2])\n",
    "DEFAULT_WGAN_PARAMS = {\n",
    "    'latent_dim': 100,\n",
    "    'lambda_gp': 10,\n",
    "    'n_critic': 5,\n",
    "    'lr': 0.00005,\n",
    "    'batch_size': 32,\n",
    "    'epochs_for_cv': 500, # K-Fold内部动态增强时使用的轮数\n",
    "    'epochs_for_final': 3000 # 用于最终增强开发集或HPO时增强开发集的轮数\n",
    "}\n",
    "\n",
    "# XGBoost RandomizedSearchCV 参数网格\n",
    "XGB_PARAM_GRID = {\n",
    "    'n_estimators': [100, 200, 300, 400],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'subsample': [0.7, 0.8, 0.9, 1],\n",
    "    'colsample_bytree': [0.7, 0.8, 0.9, 1],\n",
    "    'gamma': [0, 0.1, 0.2],\n",
    "    'reg_alpha': [0, 0.01, 0.1],\n",
    "    'reg_lambda': [0.5, 1, 1.5]\n",
    "}\n",
    "N_ITER_RANDOMIZED_SEARCH = 30 # RandomizedSearchCV的迭代次数\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"将使用设备: {device}\")\n",
    "\n",
    "# --- 1. WGAN-GP 模型定义 (Generator, Critic, gradient_penalty) ---\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128), nn.ReLU(),\n",
    "            nn.Linear(128, 256), nn.ReLU(),\n",
    "            nn.Linear(256, 512), nn.ReLU(),\n",
    "            nn.Linear(512, output_dim)\n",
    "        )\n",
    "    def forward(self, z):\n",
    "        return self.model(z)\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Critic, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 512), nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 256), nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(256, 128), nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "def gradient_penalty(critic_model, real_samples, fake_samples, device_in_use):\n",
    "    batch_size_gp = real_samples.size(0)\n",
    "    alpha = torch.rand(batch_size_gp, 1, device=device_in_use).expand_as(real_samples)\n",
    "    interpolates = (alpha * real_samples + ((1 - alpha) * fake_samples)).requires_grad_(True)\n",
    "    d_interpolates = critic_model(interpolates)\n",
    "    fake_grad_output = torch.ones_like(d_interpolates, device=device_in_use)\n",
    "    gradients = torch.autograd.grad(\n",
    "        outputs=d_interpolates, inputs=interpolates, grad_outputs=fake_grad_output,\n",
    "        create_graph=True, retain_graph=True, only_inputs=True\n",
    "    )[0]\n",
    "    gradients = gradients.view(gradients.size(0), -1)\n",
    "    gradient_penalty_val = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "    return gradient_penalty_val\n",
    "\n",
    "# --- 2. WGAN-GP 训练与生成辅助函数 ---\n",
    "def train_and_generate_wgangp(input_original_df,\n",
    "                              target_col_name, # 实际在GAN中未特殊处理，但用于列名和后处理一致性\n",
    "                              wgan_hyperparams,\n",
    "                              num_samples_to_generate,\n",
    "                              current_device,\n",
    "                              fold_num_for_logging=None,\n",
    "                              output_augmented_data_path=None):\n",
    "\n",
    "    log_prefix = f\"[WGAN-GP\"\n",
    "    if isinstance(fold_num_for_logging, int):\n",
    "        log_prefix += f\" Fold {fold_num_for_logging}\"\n",
    "    elif isinstance(fold_num_for_logging, str): # For HPO or Final stages\n",
    "        log_prefix += f\" Stage {fold_num_for_logging}\"\n",
    "    log_prefix += \"]\"\n",
    "\n",
    "    print(f\"\\n{log_prefix} 开始处理，输入数据形状: {input_original_df.shape}\")\n",
    "\n",
    "    all_feature_names = input_original_df.columns.tolist()\n",
    "    original_data_values = input_original_df.values.astype(np.float32)\n",
    "\n",
    "    # (A) 标准化所有特征 (基于当前输入数据)\n",
    "    data_mean = np.mean(original_data_values, axis=0)\n",
    "    data_std = np.std(original_data_values, axis=0)\n",
    "    data_std[data_std == 0] = 1 # 防止标准差为0\n",
    "    standardized_data = (original_data_values - data_mean) / data_std\n",
    "\n",
    "    # (B) 创建DataLoader\n",
    "    data_tensor = torch.tensor(standardized_data)\n",
    "    dataset = TensorDataset(data_tensor)\n",
    "    \n",
    "    current_batch_size = min(wgan_hyperparams['batch_size'], len(dataset))\n",
    "    if current_batch_size == 0:\n",
    "        print(f\"{log_prefix} 错误：数据集为空或过小 ({len(dataset)} samples), 无法创建DataLoader。\")\n",
    "        return pd.DataFrame(columns=all_feature_names) # 返回空DataFrame\n",
    "    \n",
    "    use_drop_last = len(dataset) >= current_batch_size * 2 # 至少需要两个batch才能考虑drop_last\n",
    "    dataloader = DataLoader(dataset, batch_size=current_batch_size, shuffle=True, drop_last=use_drop_last)\n",
    "    \n",
    "    if len(dataloader) == 0 and len(dataset) > 0 : # 再次检查，如果dataloader为空但dataset不为空\n",
    "        print(f\"{log_prefix} 警告: DataLoader为空，但输入数据集不为空。可能是batch_size ({current_batch_size}) 大于样本数 ({len(dataset)})。将尝试 drop_last=False\")\n",
    "        dataloader = DataLoader(dataset, batch_size=current_batch_size, shuffle=True, drop_last=False)\n",
    "        if len(dataloader) == 0 and len(dataset) > 0:\n",
    "             print(f\"{log_prefix} 错误: DataLoader仍然为空。无法继续GAN训练。\")\n",
    "             return pd.DataFrame(columns=all_feature_names)\n",
    "\n",
    "    # 初始化模型、优化器\n",
    "    num_features = standardized_data.shape[1]\n",
    "    generator = Generator(wgan_hyperparams['latent_dim'], num_features).to(current_device)\n",
    "    critic = Critic(num_features).to(current_device)\n",
    "    optimizer_G = optim.Adam(generator.parameters(), lr=wgan_hyperparams['lr'], betas=(0.5, 0.9))\n",
    "    optimizer_C = optim.Adam(critic.parameters(), lr=wgan_hyperparams['lr'], betas=(0.5, 0.9))\n",
    "\n",
    "    print(f\"{log_prefix} 开始WGAN-GP训练 ({wgan_hyperparams['epochs']} 轮)...\")\n",
    "    critic_loss_val, generator_loss_val = torch.tensor(0.0), torch.tensor(0.0) # Default values\n",
    "    for epoch in range(wgan_hyperparams['epochs']):\n",
    "        for i, (real_samples_batch,) in enumerate(dataloader):\n",
    "            if real_samples_batch.shape[0] == 0: continue\n",
    "            real_samples_batch = real_samples_batch.to(current_device)\n",
    "            current_real_batch_size = real_samples_batch.size(0) # 使用实际的batch size\n",
    "\n",
    "            for _ in range(wgan_hyperparams['n_critic']):\n",
    "                optimizer_C.zero_grad()\n",
    "                z = torch.randn(current_real_batch_size, wgan_hyperparams['latent_dim'], device=current_device)\n",
    "                fake_samples_batch = generator(z)\n",
    "                critic_real = critic(real_samples_batch)\n",
    "                critic_fake = critic(fake_samples_batch.detach())\n",
    "                gp = gradient_penalty(critic, real_samples_batch, fake_samples_batch, current_device)\n",
    "                critic_loss = torch.mean(critic_fake) - torch.mean(critic_real) + wgan_hyperparams['lambda_gp'] * gp\n",
    "                critic_loss.backward()\n",
    "                optimizer_C.step()\n",
    "                critic_loss_val = critic_loss # Store for printing\n",
    "            \n",
    "            optimizer_G.zero_grad()\n",
    "            z = torch.randn(current_real_batch_size, wgan_hyperparams['latent_dim'], device=current_device)\n",
    "            generated_for_g_loss = generator(z)\n",
    "            generator_loss = -torch.mean(critic(generated_for_g_loss))\n",
    "            generator_loss.backward()\n",
    "            optimizer_G.step()\n",
    "            generator_loss_val = generator_loss # Store for printing\n",
    "\n",
    "        if (epoch + 1) % (max(1, wgan_hyperparams['epochs'] // 10)) == 0: # 每10%轮数打印一次\n",
    "             print(f\"{log_prefix} [Epoch {epoch+1}/{wgan_hyperparams['epochs']}] Critic Loss: {critic_loss_val.item():.4f}, Gen Loss: {generator_loss_val.item():.4f}\")\n",
    "\n",
    "    print(f\"{log_prefix} WGAN-GP训练完成。\")\n",
    "\n",
    "    print(f\"{log_prefix} 正在生成 {num_samples_to_generate} 个增强样本...\")\n",
    "    generator.eval()\n",
    "    generated_samples_list = []\n",
    "    remaining_samples = num_samples_to_generate\n",
    "    gen_batch_size = wgan_hyperparams['batch_size'] # Use GAN's batch size for generation efficiency\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        while remaining_samples > 0:\n",
    "            current_gen_size = min(gen_batch_size, remaining_samples)\n",
    "            z_generate = torch.randn(current_gen_size, wgan_hyperparams['latent_dim'], device=current_device)\n",
    "            generated_batch_std = generator(z_generate).detach().cpu().numpy()\n",
    "            generated_samples_list.append(generated_batch_std)\n",
    "            remaining_samples -= current_gen_size\n",
    "            \n",
    "    generated_standardized_data_np = np.concatenate(generated_samples_list, axis=0)\n",
    "\n",
    "\n",
    "    # 反标准化\n",
    "    generated_data_original_scale_np = generated_standardized_data_np * data_std + data_mean\n",
    "    generated_data_df = pd.DataFrame(generated_data_original_scale_np, columns=all_feature_names)\n",
    "\n",
    "    # 后处理 ('CHO' 和 np.clip)\n",
    "    # if 'CHO' in generated_data_df.columns:\n",
    "    #     cho_column_generated = generated_data_df['CHO'].values\n",
    "    #     processed_cho = np.array([CHO_LEVELS[np.abs(CHO_LEVELS - val).argmin()] for val in cho_column_generated])\n",
    "    #     generated_data_df['CHO'] = processed_cho\n",
    "\n",
    "    for col_name in all_feature_names:\n",
    "        # if col_name == 'CHO': continue # <--- 删除或注释掉这一行，让CHO也参与通用裁剪\n",
    "        original_col_values = input_original_df[col_name]\n",
    "        col_min_original = original_col_values.min()\n",
    "        col_max_original = original_col_values.max()\n",
    "        col_range = col_max_original - col_min_original\n",
    "    \n",
    "        # 这部分原有的1%范围扩展逻辑是好的，保持不变\n",
    "        clip_min_for_col = col_min_original - 0.01 * col_range if col_range != 0 else col_min_original\n",
    "        clip_max_for_col = col_max_original + 0.01 * col_range if col_range != 0 else col_max_original\n",
    "    \n",
    "        # 在这里添加针对 CHO 和 PRO（以及其他您认为需要非负的列）的特殊处理\n",
    "        if col_name in ['CHO', 'PRO']: # 如果有其他列也需要确保非负，可以加入此列表\n",
    "            clip_min_for_col = max(0, clip_min_for_col)\n",
    "    \n",
    "        generated_data_df[col_name] = np.clip(generated_data_df[col_name], clip_min_for_col, clip_max_for_col)\n",
    "    print(f\"{log_prefix} 后处理完成。\")\n",
    "\n",
    "    if output_augmented_data_path:\n",
    "        try:\n",
    "            os.makedirs(os.path.dirname(output_augmented_data_path), exist_ok=True)\n",
    "            generated_data_df.to_excel(output_augmented_data_path, index=False)\n",
    "            print(f\"{log_prefix} 增强数据已保存到: {output_augmented_data_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"{log_prefix} 保存增强数据时发生错误: {e}\")\n",
    "            \n",
    "    return generated_data_df\n",
    "\n",
    "# --- 3. 主流程开始 ---\n",
    "try:\n",
    "    development_df_original = pd.read_excel(DEV_SET_FILE)\n",
    "    final_test_df_original = pd.read_excel(TEST_SET_FILE)\n",
    "    print(f\"开发集形状: {development_df_original.shape}, 最终测试集形状: {final_test_df_original.shape}\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"错误: 开发集或测试集文件未找到。请检查路径: {e}\")\n",
    "    exit()\n",
    "\n",
    "X_dev_original = development_df_original.drop(columns=[TARGET_COLUMN])\n",
    "y_dev_original = development_df_original[TARGET_COLUMN]\n",
    "X_final_test = final_test_df_original.drop(columns=[TARGET_COLUMN])\n",
    "y_final_test = final_test_df_original[TARGET_COLUMN]\n",
    "\n",
    "# --- 步骤三：Part 1 - 为XGBoost确定最佳超参数 ---\n",
    "print(\"\\n--- 步骤三：Part 1 - XGBoost 超参数调优 ---\")\n",
    "print(\"为超参数调优生成开发集的增强版本...\")\n",
    "num_augmented_samples_for_hpo = len(development_df_original) * 1\n",
    "current_wgan_hpo_params = DEFAULT_WGAN_PARAMS.copy()\n",
    "current_wgan_hpo_params['epochs'] = DEFAULT_WGAN_PARAMS['epochs_for_final']\n",
    "augmented_dev_for_hpo_output_path = os.path.join(AUGMENTED_DATA_OUTPUT_FOLDER, \"augmented_dev_for_hpo.xlsx\")\n",
    "\n",
    "augmented_dev_for_hpo_df = train_and_generate_wgangp(\n",
    "    input_original_df=development_df_original.copy(),\n",
    "    target_col_name=TARGET_COLUMN,\n",
    "    wgan_hyperparams=current_wgan_hpo_params,\n",
    "    num_samples_to_generate=num_augmented_samples_for_hpo,\n",
    "    current_device=device,\n",
    "    fold_num_for_logging=\"HPO_Dev_Set\",\n",
    "    output_augmented_data_path=augmented_dev_for_hpo_output_path\n",
    ")\n",
    "if augmented_dev_for_hpo_df.empty:\n",
    "    print(\"错误：为HPO生成的增强数据为空，无法继续进行超参数调优。请检查WGAN-GP流程。\")\n",
    "    exit()\n",
    "\n",
    "X_augmented_dev_for_hpo = augmented_dev_for_hpo_df.drop(columns=[TARGET_COLUMN])\n",
    "y_augmented_dev_for_hpo = augmented_dev_for_hpo_df[TARGET_COLUMN]\n",
    "X_combined_dev_for_hpo = pd.concat([X_dev_original, X_augmented_dev_for_hpo], ignore_index=True)\n",
    "y_combined_dev_for_hpo = pd.concat([y_dev_original, y_augmented_dev_for_hpo], ignore_index=True)\n",
    "print(f\"用于HPO的总数据形状: {X_combined_dev_for_hpo.shape}\")\n",
    "\n",
    "xgb_regressor_for_hpo = xgb.XGBRegressor(objective='reg:squarederror', random_state=RANDOM_STATE, tree_method='gpu_hist' if device.type == 'cuda' else 'hist')\n",
    "random_search_hpo = RandomizedSearchCV(\n",
    "    estimator=xgb_regressor_for_hpo, param_distributions=XGB_PARAM_GRID,\n",
    "    n_iter=N_ITER_RANDOMIZED_SEARCH, cv=N_SPLITS_KFOLD,\n",
    "    scoring='neg_mean_absolute_error', verbose=1, random_state=RANDOM_STATE, n_jobs=-1\n",
    ")\n",
    "print(\"开始XGBoost超参数搜索 (RandomizedSearchCV)...\")\n",
    "random_search_hpo.fit(X_combined_dev_for_hpo, y_combined_dev_for_hpo)\n",
    "best_overall_xgboost_params = random_search_hpo.best_params_\n",
    "print(f\"找到的最佳XGBoost超参数: {best_overall_xgboost_params}\")\n",
    "print(f\"最佳HPO MAE (负值): {random_search_hpo.best_score_}\")\n",
    "\n",
    "# --- 步骤三：Part 2 - K-折交叉验证与动态增强 ---\n",
    "print(f\"\\n--- 步骤三：Part 2 - 在开发集上进行 {N_SPLITS_KFOLD}-折交叉验证 (动态WGAN-GP增强) ---\")\n",
    "kf = KFold(n_splits=N_SPLITS_KFOLD, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "kfold_cv_val_metrics_list = []\n",
    "kfold_cv_train_metrics_list = []\n",
    "cv_train_mae_curves = []\n",
    "cv_val_mae_curves = []\n",
    "\n",
    "current_wgan_cv_params = DEFAULT_WGAN_PARAMS.copy()\n",
    "current_wgan_cv_params['epochs'] = DEFAULT_WGAN_PARAMS['epochs_for_cv']\n",
    "augmentation_factor_cv = 1\n",
    "\n",
    "for fold_idx, (train_indices, val_indices) in enumerate(kf.split(development_df_original)):\n",
    "    print(f\"\\n--- K-Fold: 第 {fold_idx + 1}/{N_SPLITS_KFOLD} 折 ---\")\n",
    "    cv_train_original_fold_df = development_df_original.iloc[train_indices]\n",
    "    cv_val_original_fold_df = development_df_original.iloc[val_indices]\n",
    "\n",
    "    X_cv_val_fold = cv_val_original_fold_df.drop(columns=[TARGET_COLUMN])\n",
    "    y_cv_val_fold = cv_val_original_fold_df[TARGET_COLUMN]\n",
    "\n",
    "    print(f\"当前CV训练集（原始）形状: {cv_train_original_fold_df.shape}\")\n",
    "    num_augmented_samples_cv_fold = len(cv_train_original_fold_df) * augmentation_factor_cv\n",
    "    dynamic_wgan_cv_params = current_wgan_cv_params.copy()\n",
    "    dynamic_wgan_cv_params['batch_size'] = min(current_wgan_cv_params['batch_size'], max(1, len(cv_train_original_fold_df) // 2 if len(cv_train_original_fold_df) > 1 else 1) )\n",
    "    \n",
    "    cv_augmented_fold_df = train_and_generate_wgangp(\n",
    "        input_original_df=cv_train_original_fold_df.copy(),\n",
    "        target_col_name=TARGET_COLUMN,\n",
    "        wgan_hyperparams=dynamic_wgan_cv_params,\n",
    "        num_samples_to_generate=num_augmented_samples_cv_fold,\n",
    "        current_device=device,\n",
    "        fold_num_for_logging=(fold_idx + 1),\n",
    "        output_augmented_data_path=None # CV内部一般不保存\n",
    "    )\n",
    "    if cv_augmented_fold_df.empty:\n",
    "        print(f\"警告：Fold {fold_idx + 1} 的增强数据为空，跳过此折。\")\n",
    "        cv_train_mae_curves.append([]) # 添加空列表以保持长度一致性\n",
    "        cv_val_mae_curves.append([])\n",
    "        continue\n",
    "\n",
    "    X_cv_train_original_fold = cv_train_original_fold_df.drop(columns=[TARGET_COLUMN])\n",
    "    y_cv_train_original_fold = cv_train_original_fold_df[TARGET_COLUMN]\n",
    "    X_cv_augmented_fold = cv_augmented_fold_df.drop(columns=[TARGET_COLUMN])\n",
    "    y_cv_augmented_fold = cv_augmented_fold_df[TARGET_COLUMN]\n",
    "\n",
    "    X_cv_train_combined_fold = pd.concat([X_cv_train_original_fold, X_cv_augmented_fold], ignore_index=True)\n",
    "    y_cv_train_combined_fold = pd.concat([y_cv_train_original_fold, y_cv_augmented_fold], ignore_index=True)\n",
    "    print(f\"当前CV训练集（原始+增强后）形状: {X_cv_train_combined_fold.shape}\")\n",
    "\n",
    "    model_fold = xgb.XGBRegressor(\n",
    "        **best_overall_xgboost_params, objective='reg:squarederror',\n",
    "        random_state=RANDOM_STATE, tree_method='gpu_hist' if device.type == 'cuda' else 'hist'\n",
    "    )\n",
    "    \n",
    "    eval_set_fold = [(X_cv_train_combined_fold, y_cv_train_combined_fold), (X_cv_val_fold, y_cv_val_fold)]\n",
    "    \n",
    "    print(f\"Fold {fold_idx + 1}: 开始训练XGBoost模型...\")\n",
    "    model_fold.fit(X_cv_train_combined_fold, y_cv_train_combined_fold,\n",
    "                   eval_metric='mae', eval_set=eval_set_fold,\n",
    "                   early_stopping_rounds=10, verbose=False)\n",
    "\n",
    "    fold_eval_results = model_fold.evals_result()\n",
    "    cv_train_mae_curves.append(fold_eval_results['validation_0']['mae'])\n",
    "    cv_val_mae_curves.append(fold_eval_results['validation_1']['mae'])\n",
    "\n",
    "    y_pred_val_fold = model_fold.predict(X_cv_val_fold)\n",
    "    mae_val = mean_absolute_error(y_cv_val_fold, y_pred_val_fold)\n",
    "    rmse_val = np.sqrt(mean_squared_error(y_cv_val_fold, y_pred_val_fold))\n",
    "    r2_val = r2_score(y_cv_val_fold, y_pred_val_fold)\n",
    "    kfold_cv_val_metrics_list.append({'fold': fold_idx + 1, 'MAE': mae_val, 'RMSE': rmse_val, 'R2': r2_val})\n",
    "\n",
    "    y_pred_train_fold = model_fold.predict(X_cv_train_combined_fold)\n",
    "    mae_train = mean_absolute_error(y_cv_train_combined_fold, y_pred_train_fold)\n",
    "    rmse_train = np.sqrt(mean_squared_error(y_cv_train_combined_fold, y_pred_train_fold))\n",
    "    r2_train = r2_score(y_cv_train_combined_fold, y_pred_train_fold)\n",
    "    kfold_cv_train_metrics_list.append({'fold': fold_idx + 1, 'MAE': mae_train, 'RMSE': rmse_train, 'R2': r2_train})\n",
    "    \n",
    "    print(f\"Fold {fold_idx + 1} - CV Train MAE: {mae_train:.4f}, R2: {r2_train:.4f} | CV Val MAE: {mae_val:.4f}, R2: {r2_val:.4f}\")\n",
    "\n",
    "avg_kfold_cv_val_metrics_df = pd.DataFrame(kfold_cv_val_metrics_list)\n",
    "print(\"\\n--- K-折交叉验证平均CV验证性能 (开发集, WGAN-GP动态增强) ---\")\n",
    "if not avg_kfold_cv_val_metrics_df.empty:\n",
    "    avg_mae_cv_val = avg_kfold_cv_val_metrics_df['MAE'].mean()\n",
    "    avg_rmse_cv_val = avg_kfold_cv_val_metrics_df['RMSE'].mean()\n",
    "    avg_r2_cv_val = avg_kfold_cv_val_metrics_df['R2'].mean()\n",
    "    print(f\"平均 CV 验证集 MAE: {avg_mae_cv_val:.4f}\")\n",
    "    print(f\"平均 CV 验证集 RMSE: {avg_rmse_cv_val:.4f}\")\n",
    "    print(f\"平均 CV 验证集 R2: {avg_r2_cv_val:.4f}\")\n",
    "else:\n",
    "    print(\"K-Fold CV验证结果为空，无法计算平均值。\")\n",
    "    avg_mae_cv_val, avg_r2_cv_val = np.nan, np.nan\n",
    "\n",
    "avg_kfold_cv_train_metrics_df = pd.DataFrame(kfold_cv_train_metrics_list)\n",
    "print(\"\\n--- K-折交叉验证平均CV训练性能 (开发集, WGAN-GP动态增强) ---\")\n",
    "if not avg_kfold_cv_train_metrics_df.empty:\n",
    "    avg_mae_cv_train = avg_kfold_cv_train_metrics_df['MAE'].mean()\n",
    "    avg_rmse_cv_train = avg_kfold_cv_train_metrics_df['RMSE'].mean()\n",
    "    avg_r2_cv_train = avg_kfold_cv_train_metrics_df['R2'].mean()\n",
    "    print(f\"平均 CV 训练集 MAE: {avg_mae_cv_train:.4f}\")\n",
    "    print(f\"平均 CV 训练集 R2: {avg_r2_cv_train:.4f}\")\n",
    "else:\n",
    "    print(\"K-Fold CV训练结果为空，无法计算平均值。\")\n",
    "    avg_mae_cv_train, avg_r2_cv_train = np.nan, np.nan\n",
    "\n",
    "if not np.isnan(avg_mae_cv_val) and not np.isnan(avg_mae_cv_train) :\n",
    "    print(\"\\n--- 步骤三 Part 2 结束：生成K-Fold性能图表 ---\")\n",
    "    metrics_plot_names_en = ['MAE', 'R2 Score']\n",
    "    values_cv_val_plot = [avg_mae_cv_val, avg_r2_cv_val]\n",
    "    values_cv_train_plot = [avg_mae_cv_train, avg_r2_cv_train]\n",
    "    x_axis_plot = np.arange(len(metrics_plot_names_en))\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(x_axis_plot - 0.2, values_cv_train_plot, width=0.4, label='CV Train Avg.', align='center')\n",
    "    plt.bar(x_axis_plot + 0.2, values_cv_val_plot, width=0.4, label='CV Validation Avg.', align='center')\n",
    "    plt.xticks(x_axis_plot, metrics_plot_names_en)\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('Average K-Fold CV Train vs. CV Validation Metrics (WGAN-GP Augmented)')\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plot_filename_metrics = os.path.join(OUTPUT_PLOT_PATH, \"kfold_avg_eval_metrics_wgangp.png\")\n",
    "    try:\n",
    "        plt.savefig(plot_filename_metrics, dpi=300, bbox_inches='tight')\n",
    "        print(f\"图表1已保存到: {plot_filename_metrics}\")\n",
    "    except Exception as e:\n",
    "        print(f\"保存图表1时发生错误: {e}\")\n",
    "    plt.show()\n",
    "\n",
    "    if cv_train_mae_curves and cv_val_mae_curves and \\\n",
    "       any(cv_train_mae_curves) and any(cv_val_mae_curves): # 确保有曲线数据\n",
    "        plt.figure(figsize=(14, 7))\n",
    "        for i in range(len(cv_train_mae_curves)):\n",
    "            if cv_train_mae_curves[i] and cv_val_mae_curves[i]:\n",
    "                plt.plot(cv_train_mae_curves[i], label=f'CV Train Fold {i+1}', linestyle='-')\n",
    "                plt.plot(cv_val_mae_curves[i], label=f'CV Validation Fold {i+1}', linestyle='--')\n",
    "        plt.xlabel('Boosting Round')\n",
    "        plt.ylabel('Mean Absolute Error (MAE)')\n",
    "        plt.title('Training and CV Validation MAE per Fold (WGAN-GP Augmented)')\n",
    "        if len(cv_train_mae_curves) <= 5 : plt.legend()\n",
    "        else: print(\"图例条目过多（超过5折），未在图上显示以保持清晰。\")\n",
    "        plt.grid(True, linestyle='--', alpha=0.7)\n",
    "        plot_filename_loss = os.path.join(OUTPUT_PLOT_PATH, \"kfold_mae_loss_per_fold_wgangp.png\")\n",
    "        try:\n",
    "            plt.savefig(plot_filename_loss, dpi=300, bbox_inches='tight')\n",
    "            print(f\"图表2已保存到: {plot_filename_loss}\")\n",
    "        except Exception as e:\n",
    "            print(f\"保存图表2时发生错误: {e}\")\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"没有收集到足够的MAE曲线数据用于绘制图表2。\")\n",
    "else:\n",
    "    print(\"由于K-Fold平均结果为空或NaN，跳过图表绘制。\")\n",
    "\n",
    "# --- 步骤四：训练最终模型 ---\n",
    "print(\"\\n--- 步骤四：训练最终模型 ---\")\n",
    "print(\"为最终模型生成开发集的完整增强版本...\")\n",
    "final_wgan_params = DEFAULT_WGAN_PARAMS.copy()\n",
    "final_wgan_params['epochs'] = DEFAULT_WGAN_PARAMS['epochs_for_final']\n",
    "num_augmented_samples_final = len(development_df_original) * 2\n",
    "final_augmented_dev_output_path = os.path.join(AUGMENTED_DATA_OUTPUT_FOLDER, \"augmented_dev_for_final_model.xlsx\")\n",
    "\n",
    "final_augmented_dev_df = train_and_generate_wgangp(\n",
    "    input_original_df=development_df_original.copy(),\n",
    "    target_col_name=TARGET_COLUMN,\n",
    "    wgan_hyperparams=final_wgan_params,\n",
    "    num_samples_to_generate=num_augmented_samples_final,\n",
    "    current_device=device,\n",
    "    fold_num_for_logging=\"Final_Dev_Set_Augmentation\",\n",
    "    output_augmented_data_path=final_augmented_dev_output_path\n",
    ")\n",
    "if final_augmented_dev_df.empty:\n",
    "    print(\"错误：为最终模型生成的增强数据为空，无法继续。请检查WGAN-GP流程。\")\n",
    "    exit()\n",
    "\n",
    "X_final_augmented_dev = final_augmented_dev_df.drop(columns=[TARGET_COLUMN])\n",
    "y_final_augmented_dev = final_augmented_dev_df[TARGET_COLUMN]\n",
    "X_train_final_model = pd.concat([X_dev_original, X_final_augmented_dev], ignore_index=True)\n",
    "y_train_final_model = pd.concat([y_dev_original, y_final_augmented_dev], ignore_index=True)\n",
    "print(f\"用于训练最终模型的总数据形状: {X_train_final_model.shape}\")\n",
    "\n",
    "final_model = xgb.XGBRegressor(\n",
    "    **best_overall_xgboost_params, objective='reg:squarederror',\n",
    "    random_state=RANDOM_STATE, tree_method='gpu_hist' if device.type == 'cuda' else 'hist'\n",
    ")\n",
    "print(\"开始训练最终模型...\")\n",
    "# 为了在最终模型上也能使用early_stopping，并观察其学习曲线，我们可以从最终训练集中划分一小部分作为临时验证集\n",
    "# 这不会影响最终测试集的纯洁性\n",
    "if len(X_train_final_model) > 10 : # 确保有足够数据划分\n",
    "    X_fm_train, X_fm_val, y_fm_train, y_fm_val = train_test_split(\n",
    "        X_train_final_model, y_train_final_model, test_size=0.1, random_state=RANDOM_STATE\n",
    "    )\n",
    "    eval_set_final = [(X_fm_train, y_fm_train), (X_fm_val, y_fm_val)]\n",
    "    final_model.fit(X_fm_train, y_fm_train,\n",
    "                    eval_metric='mae', eval_set=eval_set_final,\n",
    "                    early_stopping_rounds=10, verbose=False)\n",
    "    # 可选: 在早停后，用全部数据再训练一次，使用找到的最佳迭代次数 (如果需要)\n",
    "    # print(f\"最终模型早停轮数: {final_model.best_iteration}\")\n",
    "    # final_model.fit(X_train_final_model, y_train_final_model, xgb_model=final_model.get_booster()) # 这种方式可能不直接支持\n",
    "    # 或者重新初始化并训练到 best_iteration\n",
    "    # final_model = xgb.XGBRegressor(**best_overall_xgboost_params, n_estimators=final_model.best_iteration, ...)\n",
    "    # final_model.fit(X_train_final_model, y_train_final_model)\n",
    "\n",
    "else: # 数据太少，不使用早停的验证集\n",
    "    final_model.fit(X_train_final_model, y_train_final_model)\n",
    "\n",
    "\n",
    "print(\"最终模型训练完成。\")\n",
    "final_model_path = os.path.join(MODEL_OUTPUT_PATH, \"final_xgboost_wgangp_model.joblib\")\n",
    "joblib.dump(final_model, final_model_path)\n",
    "print(f\"最终模型已保存到: {final_model_path}\")\n",
    "\n",
    "print(\"\\n--- 步骤四结束：生成最终模型特征重要性图 ---\")\n",
    "feature_importances = final_model.feature_importances_\n",
    "# 确保特征名称来自原始开发集（因为增强数据列名和顺序应一致）\n",
    "importance_df = pd.DataFrame({'Feature': X_dev_original.columns, 'Importance': feature_importances})\n",
    "importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "num_features_to_plot = min(20, len(X_dev_original.columns))\n",
    "plot_height = max(6, num_features_to_plot * 0.4)\n",
    "plt.figure(figsize=(10, plot_height))\n",
    "plt.barh(importance_df['Feature'][:num_features_to_plot], importance_df['Importance'][:num_features_to_plot])\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.title(f'Top {num_features_to_plot} Feature Importances (Final Model with WGAN-GP Augmentation)')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.grid(True, axis='x', linestyle='--', alpha=0.7)\n",
    "plot_filename_importance = os.path.join(OUTPUT_PLOT_PATH, \"final_model_feature_importances_wgangp.png\")\n",
    "try:\n",
    "    plt.savefig(plot_filename_importance, dpi=300, bbox_inches='tight')\n",
    "    print(f\"最终模型特征重要性图已保存到: {plot_filename_importance}\")\n",
    "except Exception as e:\n",
    "    print(f\"保存最终模型特征重要性图时发生错误: {e}\")\n",
    "plt.show()\n",
    "\n",
    "# --- 步骤五：最终无偏评估 (在“最终测试集”上) ---\n",
    "print(\"\\n--- 步骤五：在最终测试集上进行无偏评估 ---\")\n",
    "# final_model = joblib.load(final_model_path) # 如果脚本分步执行，则取消此行注释\n",
    "y_pred_final_test = final_model.predict(X_final_test)\n",
    "mae_final = mean_absolute_error(y_final_test, y_pred_final_test)\n",
    "rmse_final = np.sqrt(mean_squared_error(y_final_test, y_pred_final_test))\n",
    "r2_final = r2_score(y_final_test, y_pred_final_test)\n",
    "print(\"--- 最终模型在最终测试集上的性能 ---\")\n",
    "print(f\"MAE: {mae_final:.4f}\")\n",
    "print(f\"RMSE: {rmse_final:.4f}\")\n",
    "print(f\"R2 Score: {r2_final:.4f}\")\n",
    "\n",
    "print(\"\\n--- 步骤五结束：生成最终测试集真实值 vs 预测值图 ---\")\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.scatter(y_final_test, y_pred_final_test, alpha=0.7, edgecolors='w', linewidth=0.5)\n",
    "min_val = min(y_final_test.min(), y_pred_final_test.min())\n",
    "max_val = max(y_final_test.max(), y_pred_final_test.max())\n",
    "plt.plot([min_val, max_val], [min_val, max_val], 'k--', lw=2) # y=x 对角线\n",
    "plt.xlabel('Actual Rowing Distance')\n",
    "plt.ylabel('Predicted Rowing Distance')\n",
    "plt.title('Final Model: Actual vs. Predicted Rowing Distance (Test Set)')\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plot_filename_actual_vs_pred = os.path.join(OUTPUT_PLOT_PATH, \"final_model_actual_vs_predicted_wgangp.png\")\n",
    "try:\n",
    "    plt.savefig(plot_filename_actual_vs_pred, dpi=300, bbox_inches='tight')\n",
    "    print(f\"最终测试集真实值 vs 预测值图已保存到: {plot_filename_actual_vs_pred}\")\n",
    "except Exception as e:\n",
    "    print(f\"保存最终测试集真实值 vs 预测值图时发生错误: {e}\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n--- 整体流程执行完毕 ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f556c738",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (xgb_gpu_env)",
   "language": "python",
   "name": "xgb_gpu_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
